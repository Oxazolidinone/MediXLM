# api/v1/endpoints/chat.py

## M·ª•c ƒë√≠ch
REST API endpoints cho chat functionality. X·ª≠ l√Ω chat messages v·ªõi RAG (Retrieval-Augmented Generation), qu·∫£n l√Ω conversation history, v√† t√≠ch h·ª£p v·ªõi Qdrant vector database ƒë·ªÉ t√¨m medical knowledge li√™n quan.

## Ch·ª©c nƒÉng ch√≠nh

### 1. POST /chat/ - Send Message v√† Nh·∫≠n AI Response
- **Input**: `ChatRequest` (message, conversation_id optional, user_id)
- **Output**: `ChatResponse` (AI response, conversation_id, message_id)
- **Workflow**:
  1. T·∫°o ho·∫∑c l·∫•y existing conversation
  2. L∆∞u user message v√†o database
  3. RAG: Search medical knowledge trong Qdrant
  4. Build prompt v·ªõi knowledge context
  5. Generate response v·ªõi Ollama LLM
  6. L∆∞u assistant message v√†o database
  7. Return response

### 2. POST /chat/test-simple - Test Endpoint
- Test endpoint kh√¥ng c·∫ßn database
- Tr·∫£ v·ªÅ mock response
- D√πng ƒë·ªÉ verify API availability

### 3. GET /chat/history/{conversation_id} - L·∫•y Conversation History
- **Input**: conversation_id, skip, limit
- **Output**: List of messages (user + assistant)
- Pagination support

### 4. GET /chat/test-qdrant - Test Qdrant Connection
- Ki·ªÉm tra k·∫øt n·ªëi Qdrant
- Hi·ªÉn th·ªã collection info
- Sample 3 documents ƒë·∫ßu ti√™n
- Debug tool ƒë·ªÉ verify RAG setup

## Ki·∫øn tr√∫c ƒë·∫∑c bi·ªát

### Thread Executor Pattern (Tr√°nh Greenlet Error)
```python
def sync_chat_handler():
    # Synchronous code in thread
    session = get_sync_session()
    # ... process chat ...
    session.commit()
    session.close()

# Run in thread pool
loop = asyncio.get_event_loop()
result = await loop.run_in_executor(None, sync_chat_handler)
```

**L√Ω do**: Tr√°nh greenlet spawn error khi mix async/sync SQLAlchemy operations

## Li√™n k·∫øt v·ªõi c√°c file kh√°c

### Dependencies (Import)
- `application/use_cases/chat_use_case.py` - Business logic (ch·ªâ d√πng cho history endpoint)
- `application/dto/chat_dto.py` - Data Transfer Objects
- `infrastructure/database/connection.py` - `get_sync_session()` cho sync operations
- `infrastructure/repositories/sync_conversation_repository.py` - Sync DB operations
- `infrastructure/services/rag_service.py` - RAG search v√† formatting
- `infrastructure/vector_db/qdrant_client.py` - Vector similarity search
- `domain/entities/` - Conversation, Message, MessageRole
- `core/config/settings.py` - Ollama model config
- `api/dependencies.py` - Dependency injection (ch·ªâ cho history endpoint)

### External Services
- **Ollama**: LLM inference (localhost:11434)
- **Qdrant**: Vector similarity search (cloud)
- **PostgreSQL**: Conversation storage (localhost:5433)

### ƒê∆∞·ª£c s·ª≠ d·ª•ng b·ªüi
- Frontend chat interface
- Mobile apps
- API consumers

## T√°c ƒë·ªông n·∫øu file n√†y b·ªã x√≥a

### üî¥ CRITICAL - CHAT FUNCTIONALITY HO√ÄN TO√ÄN KH√îNG HO·∫†T ƒê·ªòNG

N·∫øu x√≥a file n√†y:

1. **Kh√¥ng c√≥ chat API**: Users kh√¥ng th·ªÉ chat v·ªõi AI
2. **Kh√¥ng c√≥ conversation management**: Kh√¥ng t·∫°o/qu·∫£n l√Ω ƒë∆∞·ª£c conversations
3. **RAG kh√¥ng ho·∫°t ƒë·ªông**: Kh√¥ng retrieve medical knowledge
4. **M·∫•t history**: Kh√¥ng xem ƒë∆∞·ª£c conversation history
5. **Debug kh√≥ khƒÉn**: M·∫•t test endpoints (test-simple, test-qdrant)
6. **Core feature down**: Chat l√† core feature c·ªßa ·ª©ng d·ª•ng

### Business Impact
- **100% loss of main functionality**
- Users kh√¥ng th·ªÉ s·ª≠ d·ª•ng h·ªá th·ªëng
- Medical consultation kh√¥ng kh·∫£ d·ª•ng

### C√°ch thay th·∫ø
C·∫ßn t·∫°o l·∫°i chat endpoints v·ªõi:
- Message handling logic
- RAG integration
- Ollama LLM integration
- Database persistence
- Thread executor pattern (critical!)

## RAG Pipeline Detail

### Step 1: User Message Processing
```python
user_message = Message.create(
    conversation_id=conversation_id,
    role=MessageRole.USER,
    content=request.message,
)
sync_conv_repo.add_message(user_message)
```

### Step 2: RAG Search (Vector Similarity)
```python
rag_service = RAGService(qdrant)
rag_result = rag_service.get_context_for_chat(request.message, max_results=3)

# rag_result = {
#     "has_knowledge": True/False,
#     "context": "formatted knowledge text",
#     "knowledge": [list of relevant docs]
# }
```

### Step 3: Build Enhanced Prompt
```python
system_prompt = "You are MediXLM, a medical AI assistant..."
if rag_context:
    system_prompt += rag_context  # Inject knowledge
```

### Step 4: LLM Generation
```python
response = ollama.chat(
    model=settings.OLLAMA_MODEL,  # qwen2.5:7b
    messages=[
        {"role": "system", "content": system_prompt},
        *conversation_history,
        {"role": "user", "content": request.message}
    ],
    options={"temperature": 0.7, "num_predict": 500}
)
```

### Step 5: Save Response
```python
assistant_message = Message.create(
    conversation_id=conversation_id,
    role=MessageRole.ASSISTANT,
    content=response_text,
)
sync_conv_repo.add_message(assistant_message)
session.commit()
```

## Error Handling

### Graceful Degradation
1. **RAG fails**: Continue without knowledge context
   ```python
   except Exception as rag_error:
       print(f"RAG failed: {rag_error}, continuing without RAG")
   ```

2. **LLM fails**: Return fallback message
   ```python
   except Exception as llm_error:
       response_text = "I apologize, but I'm having trouble..."
   ```

3. **Database fails**: Raise HTTPException 500

### Logging Strategy
- Print statements for debugging (sync context)
- Step-by-step logging: `[SYNC] 1. Creating session...`
- Full traceback on errors

## Performance Considerations

### Thread Pool Execution
- **Pros**: Tr√°nh greenlet error, stable
- **Cons**: Blocking operation trong thread
- **Trade-off**: Ch·∫•p nh·∫≠n blocking ƒë·ªÉ tr√°nh async/sync conflicts

### Database Session Management
```python
try:
    session = get_sync_session()
    # ... operations ...
    session.commit()
finally:
    session.close()  # Always cleanup
```

### RAG Performance
- Limit 3 results (configurable)
- Vector search O(log n) v·ªõi HNSW index
- Embedding cache trong Qdrant

## Best Practices

### ‚úÖ Current Implementation
- Thread executor cho sync operations
- Graceful error handling v·ªõi fallbacks
- Detailed logging cho debugging
- RAG integration v·ªõi Qdrant
- Conversation history context

### ‚ö†Ô∏è Known Issues
- Kh√¥ng s·ª≠ d·ª•ng dependency injection cho main chat endpoint (ƒë·ªÉ tr√°nh greenlet)
- Print statements thay v√¨ proper logging
- Hardcoded prompt trong code (n√™n d√πng prompt_manager)
- Kh√¥ng c√≥ rate limiting
- Kh√¥ng c√≥ authentication/authorization

### üîß Potential Improvements
- Implement streaming responses
- Add conversation title auto-generation
- Cache embeddings cho repeated queries
- Add message editing/deletion
- Implement conversation forking
