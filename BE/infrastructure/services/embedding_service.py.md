# infrastructure/services/embedding_service.py

## M·ª•c ƒë√≠ch
File n√†y implements embedding service s·ª≠ d·ª•ng SentenceTransformers. Provides text embedding generation cho semantic search trong RAG system. Uses all-MiniLM-L6-v2 model (384 dimensions) - lightweight v√† fast model cho production use.

## Ch·ª©c nƒÉng ch√≠nh

### EmbeddingService Class

#### __init__(model_name: str = None)
Initialize embedding service:
- Load model name t·ª´ settings ho·∫∑c parameter
- Load SentenceTransformer model
- Print loading info v√† dimension
- Store dimension (384)

#### embed_text(text: str) -> List[float]
Embed single text string:
- Encode text v·ªõi model
- Convert numpy array to Python list
- Return embedding vector (384 floats)

#### embed_batch(texts: List[str]) -> List[List[float]]
Embed multiple texts efficiently:
- Batch encoding v·ªõi batch_size=32
- No progress bar (show_progress_bar=False)
- Convert to list of lists
- Return embeddings

### Global Singleton

#### get_embedding_service() -> EmbeddingService
Get or create global embedding service instance:
- Lazy initialization on first call
- Return cached instance on subsequent calls
- Ensure only one model loaded trong memory

## Li√™n k·∫øt v·ªõi c√°c file kh√°c

### Dependencies
- **sentence_transformers**: SentenceTransformer - Embedding model
- **core.config**: settings - Model name v√† dimension

### ƒê∆∞·ª£c s·ª≠ d·ª•ng b·ªüi
- **infrastructure/services/rag_service.py**: Embed queries cho similarity search
- **application/use_cases/knowledge_use_case.py**: Generate embeddings cho medical knowledge
- **scripts/seed_medical_knowledge.py**: Embed medical documents

## T√°c ƒë·ªông n·∫øu file n√†y b·ªã x√≥a

### üî¥ CRITICAL - Embedding Generation Lost

N·∫øu b·ªã x√≥a:
- **RAG system ho√†n to√†n kh√¥ng ho·∫°t ƒë·ªông**: Kh√¥ng embed queries
- **Knowledge search fails**: Kh√¥ng similarity search without embeddings
- **Cannot add medical knowledge**: Need embeddings ƒë·ªÉ store trong vector DB
- **Chat quality gi·∫£m nghi√™m tr·ªçng**: No semantic search capability

### C√°ch thay th·∫ø
1. Recreate v·ªõi SentenceTransformer model
2. Use OpenAI embeddings API (costs money)
3. Use other embedding services (Cohere, etc.)

## Technical Notes

### Model Choice: all-MiniLM-L6-v2
- **Fast**: Optimized cho inference speed
- **Lightweight**: Small model size (~90MB)
- **Good quality**: Balances speed v√† quality
- **384 dimensions**: Smaller than larger models (e.g., 768, 1536)
- **Widely used**: Battle-tested trong production

### Singleton Pattern
Global instance ensures:
- Model loaded once (expensive operation)
- Shared across all services
- Memory efficient

### Batch Processing
Batch encoding significantly faster than loop:
```python
# Slow
embeddings = [embed_text(text) for text in texts]

# Fast
embeddings = embed_batch(texts)
```

## Best Practices

### Normalization
SentenceTransformer automatically normalizes embeddings cho cosine similarity.

### Caching
Consider caching embeddings cho frequently-used queries:
```python
cache_key = f"embed:{hash(text)}"
cached = cache.get(cache_key)
if not cached:
    cached = embed_text(text)
    cache.set(cache_key, cached)
```

### GPU Acceleration
Model automatically uses GPU n·∫øu available, CPU fallback.

## Future Improvements

1. **Add caching**: Cache embeddings cho common queries
2. **Support multiple models**: Allow switching embedding models
3. **Add multilingual model**: Support Vietnamese embeddings
4. **Async interface**: Async embed methods
5. **Batch optimization**: Tune batch size cho optimal performance
6. **Model quantization**: Reduce model size v·ªõi quantization
7. **Dimension reduction**: PCA/UMAP ƒë·ªÉ reduce dimensions n·∫øu needed
