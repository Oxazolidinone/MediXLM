# infrastructure/services/local_llm_service.py

## M·ª•c ƒë√≠ch
File n√†y implements local LLM service s·ª≠ d·ª•ng HuggingFace Transformers. Provides text generation v·ªõi streaming support v√† embedding generation. Uses local models (microsoft/phi-2) ƒë·ªÉ avoid API costs v√† ensure privacy. Currently partially implemented, some methods reference non-existent model attributes.

## Ch·ª©c nƒÉng ch√≠nh

### LocalLLMService Class

#### __init__()
Initialize LLM service:
- Detect device (CUDA/CPU)
- Load tokenizer t·ª´ HuggingFace
- Load model (AutoModelForCausalLM) v·ªõi appropriate dtype
- Set model to eval mode

#### _format_messages(messages, system_prompt) -> str
Format conversation messages th√†nh prompt string:
- Add system prompt n·∫øu provided
- Format messages v·ªõi role prefixes (User:, Assistant:)
- Prepare cho model input

#### generate_streaming_response(messages, temperature, max_tokens, system_prompt)
Generate streaming LLM response:
- Format messages th√†nh prompt
- Tokenize input
- Use TextIteratorStreamer cho streaming output
- Run generation trong separate thread
- Yield tokens as they're generated

#### generate_embeddings(text: str) -> List[float]
Generate embeddings cho text (currently broken - calls self.model.encode which doesn't exist for CausalLM).

#### generate_batch_embeddings(texts: List[str]) -> List[List[float]]
Generate embeddings cho batch of texts (also broken).

## Li√™n k·∫øt v·ªõi c√°c file kh√°c

### Dependencies
- **torch**: PyTorch framework
- **transformers**: AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
- **threading**: Thread cho streaming
- **core.config**: settings - Model configuration

### ƒê∆∞·ª£c s·ª≠ d·ª•ng b·ªüi
- **application/use_cases/chat_use_case.py**: LLM inference (currently disabled)
- **api/dependencies.py**: Dependency injection

## T√°c ƒë·ªông n·∫øu file n√†y b·ªã x√≥a

### üü° HIGH - Local LLM Service Lost

N·∫øu b·ªã x√≥a:
- **Local LLM kh√¥ng ho·∫°t ƒë·ªông**: Ph·∫£i d√πng external API (Ollama, OpenAI)
- **Chat responses b·ªã ·∫£nh h∆∞·ªüng**: Currently disabled trong use case, so minimal immediate impact
- **Privacy option lost**: Kh√¥ng option ƒë·ªÉ run completely local

### C√°ch thay th·∫ø
1. Use Ollama service instead (already configured)
2. Use cloud LLM APIs (OpenAI, Anthropic)
3. Recreate v·ªõi proper model loading

## Technical Notes

### Issues
- **Embedding methods broken**: CausalLM models kh√¥ng c√≥ .encode() method. C·∫ßn SentenceTransformer model cho embeddings.
- **Currently unused**: Chat use case kh√¥ng call LLM service (disabled cho debugging)

### Model Choice
- **phi-2**: Small efficient model (2.7B parameters)
- Lightweight enough cho CPU inference
- Good cho testing v√† development

### Streaming Implementation
Uses thread-based streaming:
- TextIteratorStreamer collects generated tokens
- Model.generate runs trong separate thread
- Main thread yields tokens as available

## Future Improvements

1. **Fix embedding methods**: Use separate embedding model ho·∫∑c remove methods
2. **Add error handling**: Handle model loading errors, OOM errors
3. **Add batching**: Batch multiple requests cho efficiency
4. **Add caching**: Cache common responses
5. **Integrate with chat use case**: Re-enable LLM calls
6. **Support multiple models**: Allow switching models dynamically
7. **Add quantization**: Use 4-bit/8-bit quantization cho memory efficiency
