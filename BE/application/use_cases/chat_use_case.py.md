# application/use_cases/chat_use_case.py

## M·ª•c ƒë√≠ch
File n√†y ch·ª©a business logic ch√≠nh cho t√≠nh nƒÉng chat c·ªßa h·ªá th·ªëng. ChatUseCase x·ª≠ l√Ω to√†n b·ªô flow t·ª´ nh·∫≠n tin nh·∫Øn ng∆∞·ªùi d√πng, truy v·∫•n knowledge graph, g·ªçi LLM, ƒë·∫øn l∆∞u tr·ªØ conversation v√†o database. ƒê√¢y l√† core use case orchestrating nhi·ªÅu services v√† repositories.

## Ch·ª©c nƒÉng ch√≠nh

### ChatUseCase Class
Orchestrates chat flow v·ªõi c√°c dependencies:
- **conversation_repository**: Qu·∫£n l√Ω conversations v√† messages
- **knowledge_graph_repository**: Truy v·∫•n medical knowledge graph
- **cache_repository**: Cache responses v√† knowledge
- **llm_service**: G·ªçi Local LLM ƒë·ªÉ generate responses
- **embedding_service**: Generate embeddings cho semantic search

### process_message(request: ChatRequestDTO) -> ChatResponseDTO
X·ª≠ l√Ω tin nh·∫Øn t·ª´ user v·ªõi flow:
1. **Get or create conversation**: L·∫•y conversation hi·ªán c√≥ ho·∫∑c t·∫°o m·ªõi
2. **Save user message**: L∆∞u tin nh·∫Øn c·ªßa user v√†o database
3. **Get conversation history**: L·∫•y 4 tin nh·∫Øn g·∫ßn nh·∫•t ƒë·ªÉ l√†m context
4. **Knowledge retrieval** (commented out): T√¨m ki·∫øm medical knowledge li√™n quan
5. **Generate response**: G·ªçi LLM ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi
6. **Save assistant message**: L∆∞u ph·∫£n h·ªìi c·ªßa AI v√†o database
7. **Return response**: Tr·∫£ v·ªÅ ChatResponseDTO

### get_conversation_history(conversation_id, skip, limit) -> List[MessageDTO]
L·∫•y l·ªãch s·ª≠ chat c·ªßa m·ªôt conversation v·ªõi pagination.

### Helper Methods
- **_build_knowledge_context**: Format knowledge graph results cho prompt
- **_build_conversation_history**: Format messages history cho LLM
- **_build_system_prompt**: T·∫°o system prompt v·ªõi knowledge context

## Li√™n k·∫øt v·ªõi c√°c file kh√°c

### Dependencies (Import)
- **application/dto**: ChatRequestDTO, ChatResponseDTO, MessageDTO
- **infrastructure/services**: LLMService
- **domain/entities**: Conversation, Message, MessageRole
- **domain/repositories**: IConversationRepository, IKnowledgeGraphRepository, ICacheRepository
- **core/exceptions**: ConversationNotFoundError
- **core/prompts**: build_chat_prompt, format_knowledge_context
- **infrastructure/database/connection**: get_sync_session
- **infrastructure/repositories/sync_conversation_repository**: SyncConversationRepository
- **infrastructure/cache/redis_client**: get_redis_client

### ƒê∆∞·ª£c s·ª≠ d·ª•ng b·ªüi
- **api/v1/endpoints/chat.py**: Chat API endpoints s·ª≠ d·ª•ng use case n√†y
- **api/dependencies.py**: Dependency injection setup

## T√°c ƒë·ªông n·∫øu file n√†y b·ªã x√≥a

### üî¥ CRITICAL - Complete Chat System Failure

ƒê√¢y l√† file QUAN TR·ªåNG NH·∫§T c·ªßa chat system. N·∫øu b·ªã x√≥a:

- **To√†n b·ªô t√≠nh nƒÉng chat ng·ª´ng ho·∫°t ƒë·ªông**: API chat endpoints s·∫Ω kh√¥ng c√≥ business logic
- **Kh√¥ng th·ªÉ x·ª≠ l√Ω tin nh·∫Øn**: Users kh√¥ng th·ªÉ chat v·ªõi AI assistant
- **M·∫•t orchestration logic**: Kh√¥ng c√≥ code ƒëi·ªÅu ph·ªëi gi·ªØa repositories, LLM, v√† knowledge graph
- **Breaking change nghi√™m tr·ªçng**: ·ª®ng d·ª•ng m·∫•t ch·ª©c nƒÉng core nh·∫•t
- **Data flow b·ªã ƒë·ª©t**: Kh√¥ng c√≥ layer k·∫øt n·ªëi API v√† infrastructure

### C√°ch thay th·∫ø
1. **T·∫°o l·∫°i use case** v·ªõi c√πng interface v√† dependencies
2. **Implement l·∫°i to√†n b·ªô business logic**:
   - Conversation management
   - Message persistence
   - LLM integration
   - Knowledge retrieval
   - Error handling
3. **Maintain dependency injection** v·ªõi c√°c repositories v√† services
4. **Keep async/sync handling** cho database operations

## Technical Notes

### Threading for Synchronous Database Operations
Use case n√†y s·ª≠ d·ª•ng `loop.run_in_executor()` ƒë·ªÉ ch·∫°y synchronous database operations trong thread pool, tr√°nh blocking async event loop:

```python
loop = asyncio.get_event_loop()
result = await loop.run_in_executor(None, sync_db_operations)
```

### Greenlet Compatibility
Code hi·ªán t·∫°i s·ª≠ d·ª•ng synchronous SQLAlchemy session trong thread ƒë·ªÉ tr√°nh greenlet spawn errors v·ªõi async SQLAlchemy.

### LLM Integration Currently Disabled
LLM call ƒëang b·ªã comment out v√† thay b·∫±ng placeholder response ƒë·ªÉ debug database issues:
```python
response_text = "Test response - LLM disabled for debugging"
```

### Transaction Management
Database operations ƒë∆∞·ª£c wrap trong try-finally block v·ªõi commit/rollback logic trong `get_sync_session()`.

## Future Improvements

1. **Re-enable LLM integration**: Uncomment LLM service calls
2. **Add RAG (Retrieval-Augmented Generation)**: Implement knowledge retrieval
3. **Implement caching**: Cache frequent queries v√† responses
4. **Add streaming responses**: Stream LLM output cho better UX
5. **Error recovery**: Better error handling v√† retry logic
6. **Performance optimization**: Optimize database queries v√† LLM calls
